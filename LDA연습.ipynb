{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244179</th>\n",
       "      <td>20211231</td>\n",
       "      <td>two aged care residents die as state records 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244180</th>\n",
       "      <td>20211231</td>\n",
       "      <td>victoria records 5;919 new cases and seven deaths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244181</th>\n",
       "      <td>20211231</td>\n",
       "      <td>wa delays adopting new close contact definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244182</th>\n",
       "      <td>20211231</td>\n",
       "      <td>western ringtail possums found badly dehydrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244183</th>\n",
       "      <td>20211231</td>\n",
       "      <td>what makes you a close covid contact here are ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1244184 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         publish_date                                      headline_text\n",
       "0            20030219  aba decides against community broadcasting lic...\n",
       "1            20030219     act fire witnesses must be aware of defamation\n",
       "2            20030219     a g calls for infrastructure protection summit\n",
       "3            20030219           air nz staff in aust strike for pay rise\n",
       "4            20030219      air nz strike to affect australian travellers\n",
       "...               ...                                                ...\n",
       "1244179      20211231  two aged care residents die as state records 2...\n",
       "1244180      20211231  victoria records 5;919 new cases and seven deaths\n",
       "1244181      20211231    wa delays adopting new close contact definition\n",
       "1244182      20211231  western ringtail possums found badly dehydrate...\n",
       "1244183      20211231  what makes you a close covid contact here are ...\n",
       "\n",
       "[1244184 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "data = pd.read_csv(\"abcnews-date-text.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 열 선택 (headline_text: 기사 텍스트, category: 카테고리)\n",
    "data = data[['headline_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제 함수 (불용어 제거와 1글자짜리 단어 제거)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # 영문자와 공백 이외의 문자 제거\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    # 불용어 제거 및 2글자 이하 단어 제거\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words and len(word) > 2)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_15212\\2934845252.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['headline_text'] = data['headline_text'].apply(clean_text)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides community broadcasting licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calls infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air staff aust strike pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air strike affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                headline_text\n",
       "0  aba decides community broadcasting licence\n",
       "1    act fire witnesses must aware defamation\n",
       "2      calls infrastructure protection summit\n",
       "3              air staff aust strike pay rise\n",
       "4     air strike affect australian travellers"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 정제 적용\n",
    "data['headline_text'] = data['headline_text'].apply(clean_text)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_15212\\2194088091.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['headline_text'] = data.apply(lambda x: nltk.word_tokenize(x['headline_text']), axis=1)\n",
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_15212\\2194088091.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['headline_text'] = data['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decide, community, broadcast, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witness, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   headline_text\n",
       "0   [aba, decide, community, broadcast, licence]\n",
       "1  [act, fire, witness, must, aware, defamation]\n",
       "2     [call, infrastructure, protection, summit]\n",
       "3          [air, staff, aust, strike, pay, rise]\n",
       "4  [air, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 동사 시제에 따라 변한 것을 동사 원형으로 만드는 장겁\n",
    "data['headline_text'] = data.apply(lambda x: nltk.word_tokenize(x['headline_text']), axis=1)\n",
    "data['headline_text'] = data['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(data)):\n",
    "    t = ' '.join(data['headline_text'][i])\n",
    "    detokenized_doc.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\AppData\\Local\\Temp\\ipykernel_15212\\821682935.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['headline_text'] = detokenized_doc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decide community broadcast licence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witness must aware defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air staff aust strike pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air strike affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             headline_text\n",
       "0   aba decide community broadcast licence\n",
       "1   act fire witness must aware defamation\n",
       "2    call infrastructure protection summit\n",
       "3           air staff aust strike pay rise\n",
       "4  air strike affect australian travellers"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다시 text['headline_text']에 재저장\n",
    "data['headline_text'] = detokenized_doc\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, max_df=0.85)\n",
    "X = tfidf_vectorizer.fit_transform(data['headline_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tokens = []\n",
    "for i in range(len(data)):\n",
    "    doc = X[i, :].toarray()\n",
    "    tokens = [tfidf_feature_names[j] for j in np.where(doc > 0)[1]]\n",
    "    tfidf_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 결과를 데이터프레임에 추가\n",
    "data['tfidf_tokens'] = tfidf_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>tfidf_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decide community broadcast licence</td>\n",
       "      <td>[aba, broadcast, community, decide, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witness must aware defamation</td>\n",
       "      <td>[act, aware, defamation, fire, must, witness]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>call infrastructure protection summit</td>\n",
       "      <td>[call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air staff aust strike pay rise</td>\n",
       "      <td>[air, aust, pay, rise, staff, strike]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air strike affect australian travellers</td>\n",
       "      <td>[affect, air, australian, strike, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             headline_text  \\\n",
       "0   aba decide community broadcast licence   \n",
       "1   act fire witness must aware defamation   \n",
       "2    call infrastructure protection summit   \n",
       "3           air staff aust strike pay rise   \n",
       "4  air strike affect australian travellers   \n",
       "\n",
       "                                    tfidf_tokens  \n",
       "0   [aba, broadcast, community, decide, licence]  \n",
       "1  [act, aware, defamation, fire, must, witness]  \n",
       "2     [call, infrastructure, protection, summit]  \n",
       "3          [air, aust, pay, rise, staff, strike]  \n",
       "4  [affect, air, australian, strike, travellers]  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary 및 corpus 생성\n",
    "dictionary = Dictionary(tfidf_tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in tfidf_tokens]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water 10000\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[66], len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"australian\" + 0.023*\"australia\" + 0.022*\"trump\" + 0.016*\"coronavirus\" + 0.016*\"test\" + 0.015*\"vaccine\" + 0.014*\"melbourne\" + 0.013*\"live\" + 0.013*\"border\" + 0.011*\"day\"')\n",
      "(1, '0.032*\"police\" + 0.028*\"queensland\" + 0.013*\"nsw\" + 0.012*\"two\" + 0.012*\"coast\" + 0.011*\"kill\" + 0.011*\"sydney\" + 0.011*\"find\" + 0.010*\"crash\" + 0.010*\"morrison\"')\n",
      "(2, '0.073*\"covid\" + 0.024*\"coronavirus\" + 0.017*\"australia\" + 0.014*\"china\" + 0.013*\"say\" + 0.012*\"first\" + 0.012*\"state\" + 0.011*\"new\" + 0.011*\"warn\" + 0.008*\"speak\"')\n",
      "(3, '0.015*\"change\" + 0.015*\"donald\" + 0.013*\"lockdown\" + 0.013*\"say\" + 0.012*\"people\" + 0.010*\"quarantine\" + 0.010*\"health\" + 0.010*\"national\" + 0.009*\"concern\" + 0.009*\"minister\"')\n",
      "(4, '0.029*\"case\" + 0.017*\"government\" + 0.013*\"home\" + 0.013*\"court\" + 0.013*\"news\" + 0.013*\"charge\" + 0.011*\"man\" + 0.011*\"die\" + 0.010*\"face\" + 0.010*\"murder\"')\n",
      "(5, '0.030*\"new\" + 0.028*\"coronavirus\" + 0.023*\"victoria\" + 0.019*\"nsw\" + 0.016*\"record\" + 0.014*\"election\" + 0.013*\"restrictions\" + 0.011*\"death\" + 0.010*\"qld\" + 0.010*\"hit\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 6 # 6개의 토픽\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\win\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\win\\anaconda3\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'pandas.core.indexes.numeric'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\win\\Desktop\\뉴스기사분류\\LDA연습.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/win/Desktop/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98/LDA%EC%97%B0%EC%8A%B5.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgensim_models\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/win/Desktop/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98/LDA%EC%97%B0%EC%8A%B5.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/win/Desktop/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98/LDA%EC%97%B0%EC%8A%B5.ipynb#Y121sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vis \u001b[39m=\u001b[39m pyLDAvis\u001b[39m.\u001b[39mgensim_models\u001b[39m.\u001b[39mprepare(ldamodel, corpus, dictionary)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/win/Desktop/%EB%89%B4%EC%8A%A4%EA%B8%B0%EC%82%AC%EB%B6%84%EB%A5%98/LDA%EC%97%B0%EC%8A%B5.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39mdisplay(vis)\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[1;32m--> 123\u001b[0m \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39mprepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\pyLDAvis\\_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[39m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[0;32m    430\u001b[0m term_frequency \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(term_topic_freq, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 432\u001b[0m topic_info \u001b[39m=\u001b[39m _topic_info(topic_term_dists, topic_proportion,\n\u001b[0;32m    433\u001b[0m                          term_frequency, term_topic_freq, vocab, lambda_step, R,\n\u001b[0;32m    434\u001b[0m                          n_jobs, start_index)\n\u001b[0;32m    435\u001b[0m token_table \u001b[39m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[0;32m    436\u001b[0m topic_coordinates \u001b[39m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\pyLDAvis\\_prepare.py:273\u001b[0m, in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    262\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m'\u001b[39m: vocab[term_ix],\n\u001b[0;32m    263\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m'\u001b[39m: term_topic_freq\u001b[39m.\u001b[39mloc[original_topic_id, term_ix],\n\u001b[0;32m    264\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m'\u001b[39m: term_frequency[term_ix],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mloglift\u001b[39m\u001b[39m'\u001b[39m: log_lift\u001b[39m.\u001b[39mloc[original_topic_id, term_ix]\u001b[39m.\u001b[39mround(\u001b[39m4\u001b[39m),\n\u001b[0;32m    268\u001b[0m                        })\n\u001b[0;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39mreindex(columns\u001b[39m=\u001b[39m[\n\u001b[0;32m    270\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTerm\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFreq\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTotal\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlogprob\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloglift\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m     ])\n\u001b[1;32m--> 273\u001b[0m top_terms \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs)\n\u001b[0;32m    274\u001b[0m                       (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[0;32m    275\u001b[0m                       \u001b[39mfor\u001b[39;00m ls \u001b[39min\u001b[39;00m _job_chunks(lambda_seq, n_jobs)))\n\u001b[0;32m    276\u001b[0m topic_dfs \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(topic_top_term_df, \u001b[39menumerate\u001b[39m(top_terms\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39miterrows(), start_index))\n\u001b[0;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mconcat([default_term_info] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(topic_dfs))\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39mresult(timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\win\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서 별 토픽 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은 [(0, 0.20452273), (1, 0.029229345), (2, 0.029229375), (3, 0.55547667), (4, 0.029289646), (5, 0.15225221)]\n",
      "[(3, 0.55547667), (0, 0.20452273), (5, 0.15225221), (4, 0.029289646), (2, 0.029229375), (1, 0.029229345)]\n",
      "1 번째 문서의 topic 비율은 [(0, 0.3090777), (1, 0.024005834), (2, 0.023827529), (3, 0.023827529), (4, 0.59531456), (5, 0.02394689)]\n",
      "[(4, 0.59531456), (0, 0.3090777), (1, 0.024005834), (5, 0.02394689), (2, 0.023827529), (3, 0.023827529)]\n",
      "2 번째 문서의 topic 비율은 [(0, 0.033341743), (1, 0.033341743), (2, 0.26875195), (3, 0.5977276), (4, 0.0334126), (5, 0.03342436)]\n",
      "[(3, 0.5977276), (2, 0.26875195), (5, 0.03342436), (4, 0.0334126), (0, 0.033341743), (1, 0.033341743)]\n",
      "3 번째 문서의 topic 비율은 [(0, 0.023835283), (1, 0.02391323), (2, 0.3869669), (3, 0.19881777), (4, 0.023917923), (5, 0.34254885)]\n",
      "[(2, 0.3869669), (5, 0.34254885), (3, 0.19881777), (4, 0.023917923), (1, 0.02391323), (0, 0.023835283)]\n",
      "4 번째 문서의 topic 비율은 [(0, 0.19448482), (1, 0.02801508), (2, 0.028502064), (3, 0.36052954), (4, 0.36068404), (5, 0.027784444)]\n",
      "[(4, 0.36068404), (3, 0.36052954), (0, 0.19448482), (2, 0.028502064), (1, 0.02801508), (5, 0.027784444)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'번째 문서의 topic 비율은',topic_list)\n",
    "    # doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "    doc = sorted(topic_list, key=lambda x: (x[1]), reverse=True)\n",
    "    print(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        if i == 1000:\n",
    "            break\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = pd.concat([topic_table, pd.DataFrame([int(topic_num), round(prop_topic,4), doc]).T], ignore_index= True)\n",
    "                # pd.DataFrame({'가장 비중이 높은 토픽' : int(topic_num), '가장 높은 토픽의 비중' : round(prop_topic,4), '각 토픽의 비중':topic_list})\n",
    "                # print(topic_table)\n",
    "                # topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return topic_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['num', 'best_topic', 'best_topic_rate', 'topic_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>best_topic</th>\n",
       "      <th>best_topic_rate</th>\n",
       "      <th>topic_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>[(3, 0.555338), (0, 0.20447157), (5, 0.1524643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5953</td>\n",
       "      <td>[(4, 0.5953144), (0, 0.30907768), (1, 0.024005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5977</td>\n",
       "      <td>[(3, 0.5977433), (2, 0.26873627), (5, 0.033424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.387</td>\n",
       "      <td>[(2, 0.38697717), (5, 0.34254503), (3, 0.19881...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3607</td>\n",
       "      <td>[(4, 0.3606937), (3, 0.36053774), (0, 0.194484...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>[(0, 0.43279567), (3, 0.23354031), (4, 0.23351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7913</td>\n",
       "      <td>[(3, 0.79130876), (2, 0.04188438), (4, 0.04179...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3623</td>\n",
       "      <td>[(0, 0.3622772), (3, 0.3099698), (4, 0.2429436...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633</td>\n",
       "      <td>[(1, 0.6330109), (0, 0.23337057), (5, 0.033460...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>[(5, 0.4097465), (0, 0.34041208), (2, 0.177823...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num best_topic best_topic_rate  \\\n",
       "0      0          3          0.5553   \n",
       "1      1          4          0.5953   \n",
       "2      2          3          0.5977   \n",
       "3      3          2           0.387   \n",
       "4      4          4          0.3607   \n",
       "..   ...        ...             ...   \n",
       "995  995          0          0.4328   \n",
       "996  996          3          0.7913   \n",
       "997  997          0          0.3623   \n",
       "998  998          1           0.633   \n",
       "999  999          5          0.4097   \n",
       "\n",
       "                                            topic_rate  \n",
       "0    [(3, 0.555338), (0, 0.20447157), (5, 0.1524643...  \n",
       "1    [(4, 0.5953144), (0, 0.30907768), (1, 0.024005...  \n",
       "2    [(3, 0.5977433), (2, 0.26873627), (5, 0.033424...  \n",
       "3    [(2, 0.38697717), (5, 0.34254503), (3, 0.19881...  \n",
       "4    [(4, 0.3606937), (3, 0.36053774), (0, 0.194484...  \n",
       "..                                                 ...  \n",
       "995  [(0, 0.43279567), (3, 0.23354031), (4, 0.23351...  \n",
       "996  [(3, 0.79130876), (2, 0.04188438), (4, 0.04179...  \n",
       "997  [(0, 0.3622772), (3, 0.3099698), (4, 0.2429436...  \n",
       "998  [(1, 0.6330109), (0, 0.23337057), (5, 0.033460...  \n",
       "999  [(5, 0.4097465), (0, 0.34041208), (2, 0.177823...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
